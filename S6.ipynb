{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "802f26bd",
   "metadata": {
    "id": "802f26bd"
   },
   "source": [
    "## Classification of SVHN using Transfer Learning from pre-trained MNIST CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc2b18d",
   "metadata": {
    "id": "efc2b18d"
   },
   "source": [
    "### The Street View House Numbers (SVHN) Dataset\n",
    "\n",
    "SVHN is a real-world image dataset for developing machine learning and object recognition algorithms with minimal requirement on data preprocessing and formatting. It can be seen as similar in flavor to MNIST (e.g., the images are of small cropped digits), but incorporates an order of magnitude more labeled data (over 600,000 digit images) and comes from a significantly harder, real world problem (recognizing digits and numbers in natural scene images). SVHN is obtained from house numbers in Google Street View images.\n",
    "\n",
    "- 10 classes, 1 for each digit. Digit '1' has label 1, '9' has label 9 and '0' has label 0.\n",
    "- 73257 digits for training, 26032 digits for testing, and 531131 additional, somewhat less difficult samples, to use as extra training data\n",
    "- Comes in two formats:\n",
    "  1. Original images with character level bounding boxes.\n",
    "  2. MNIST-like 32-by-32 images centered around a single character (many of the images do contain some distractors at the sides).\n",
    "\n",
    "- The dataset that we will be using in this notebook contains 42000 training samples and 18000 testing samples\n",
    "\n",
    "Dataset link : https://www.kaggle.com/sasha18/street-view-house-nos-h5-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094ff7ee",
   "metadata": {
    "id": "094ff7ee"
   },
   "outputs": [],
   "source": [
    "#import tensorflow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90e266d",
   "metadata": {
    "id": "b90e266d"
   },
   "outputs": [],
   "source": [
    "#The SVHN dataset is provided in the form of h5. We can extract the data as train and test\n",
    "                                                                #using the following script. \n",
    "import h5py\n",
    "\n",
    "data=h5py.File('SVHN_single_grey1.h5','r')\n",
    "\n",
    "X_train=data['X_train'][:]\n",
    "y_train=data['y_train'][:]\n",
    "\n",
    "X_test=data['X_test'][:]\n",
    "y_test=data['y_test'][:]\n",
    "\n",
    "data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4811e5b",
   "metadata": {
    "id": "f4811e5b"
   },
   "outputs": [],
   "source": [
    "#scaling the data\n",
    "x_train=X_train/255.\n",
    "x_test=X_test/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f768d7",
   "metadata": {
    "id": "a9f768d7",
    "outputId": "07b598cd-b997-4562-b67f-eef823e97877"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 32, 32)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a96504",
   "metadata": {
    "id": "82a96504",
    "outputId": "2cdc54a0-a1b0-48da-aee5-2aad84b220e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x23306cc0828>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x_train[20,:,:],cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3161eb56",
   "metadata": {
    "id": "3161eb56",
    "outputId": "18f9a3fe-3206-4d96-9b3f-399407458c32"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc60992",
   "metadata": {
    "id": "7cc60992"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b99bc9c",
   "metadata": {
    "id": "6b99bc9c"
   },
   "outputs": [],
   "source": [
    "# Resize the images as 28x28 \n",
    "#(MNIST dataset size is 28x28, as we are using the MNIST model weight, \n",
    "                                                     #we need to resize these images as 28x28 )\n",
    "import cv2\n",
    "import numpy as np\n",
    "X_train_resize=np.zeros((42000,28,28))\n",
    "\n",
    "for i in range(42000):\n",
    "    X_train_resize[i,:,:]=cv2.resize(x_train[i],dsize=(28,28))\n",
    "\n",
    "X_test_resize=np.zeros((18000,28,28))\n",
    "for i in range(18000):\n",
    "    X_test_resize[i,:,:]=cv2.resize(x_test[i],dsize=(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06e3feb",
   "metadata": {
    "id": "e06e3feb",
    "outputId": "df5e955b-22fe-47d8-91e4-9022c08271c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 28, 28)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_resize.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c38c7b7",
   "metadata": {
    "id": "3c38c7b7"
   },
   "outputs": [],
   "source": [
    "#Reshape the train and test datasets to make them 4-D\n",
    "xtrain1=X_train_resize.reshape(42000,28,28,1) # u can also use np.expand_dim\n",
    "xtest1=X_test_resize.reshape(18000,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f9ffab",
   "metadata": {
    "id": "23f9ffab",
    "outputId": "dcaea638-91b3-4c61-ebad-105eec9616c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Encoding the target variable\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "ytrain=to_categorical(y_train,num_classes=10)\n",
    "ytest=to_categorical(y_test,num_classes=10)\n",
    "ytrain[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334f3c4d",
   "metadata": {
    "id": "334f3c4d"
   },
   "outputs": [],
   "source": [
    "# Import the libraries required for CNN\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,Flatten,Dense,Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f552db71",
   "metadata": {
    "id": "f552db71"
   },
   "outputs": [],
   "source": [
    "# The CNN model architecture must be same as the one we have builded for MNIST dataset,\n",
    "# because our idea is to apply the weights trained for MNIST dataset to apply here on SVHN data model\n",
    "classifier1=Sequential()\n",
    "\n",
    "classifier1.add(Conv2D(16,(3,3),input_shape=(28,28,1),activation ='relu'))\n",
    "classifier1.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "classifier1.add(Conv2D(32,(3,3),activation ='relu'))\n",
    "classifier1.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "classifier1.add(Flatten())\n",
    "\n",
    "classifier1.add(Dense(units=64,activation='relu'))\n",
    "classifier1.add(Dense(units=10,activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0d37ee",
   "metadata": {
    "id": "af0d37ee"
   },
   "outputs": [],
   "source": [
    "#Apply the MNIST data model weight on the above CNN architecture\n",
    "classifier1.load_weights('my_digit_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b198bc4",
   "metadata": {
    "id": "2b198bc4"
   },
   "outputs": [],
   "source": [
    "# Compilation statergy\n",
    "classifier1.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62708e94",
   "metadata": {
    "id": "62708e94",
    "outputId": "cf33ac69-ff52-4237-d357-6ba260e06f75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 2s 133us/sample - loss: 2.6429 - accuracy: 0.2174\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.642934987809923, 0.21744445]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluating the \"SVHN digit\" test data on the model with the \"MNIST digit\" weights\n",
    "classifier1.evaluate(xtest1,ytest)\n",
    "#This model producing the accuracy of around 22 percent only as the two dataset are differnt (eventhough both contains digit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d1f2b9",
   "metadata": {
    "id": "b0d1f2b9"
   },
   "outputs": [],
   "source": [
    "# To improve the performance of this model, we need to re-train some section of the weights \n",
    "# with respect to SVHN dataset.\n",
    "\n",
    "#One approach is keep the weights of the convolution layer as it is, but re-train the weights of the \n",
    "#dense layers (hidden and output layer)\n",
    "# This is know as Transfer Learning through feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450619d6",
   "metadata": {
    "id": "450619d6",
    "outputId": "c2777204-e271-4a4f-9e6f-f31b7cc37ca9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.convolutional.Conv2D at 0x233049c3a58>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x233049c3b70>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x233049c84a8>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x233049c8a58>,\n",
       " <tensorflow.python.keras.layers.core.Flatten at 0x233049d2128>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x233049d2518>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x233049c3f98>]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier1.layers #accessing the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff41ff35",
   "metadata": {
    "id": "ff41ff35",
    "outputId": "0dd24a38-1f94-442f-fe42-cff7b7cb0106"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_12\n",
      "max_pooling2d_12\n",
      "conv2d_13\n",
      "max_pooling2d_13\n",
      "flatten_5\n",
      "dense_10\n",
      "dense_11\n"
     ]
    }
   ],
   "source": [
    "#Accessing the layer name\n",
    "for layer in classifier1.layers:\n",
    "    print(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c473b5",
   "metadata": {
    "id": "45c473b5",
    "outputId": "4d6cce39-79c3-4e15-db1c-5209e9697c5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Trainability nature of each layer.\n",
    "#by default all layer weights are trainable\n",
    "for layer in classifier1.layers:\n",
    "    print(layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6956c140",
   "metadata": {
    "id": "6956c140"
   },
   "outputs": [],
   "source": [
    "#freeze the convolution layer and train only the dense layer\n",
    "for layer in classifier1.layers:\n",
    "    if ('dense' not in layer.name):\n",
    "        layer.trainable=False\n",
    "    if('dense' in layer.name):\n",
    "        layer.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f50c243",
   "metadata": {
    "id": "4f50c243",
    "outputId": "68293bc9-94da-4e9b-bda9-e5ce6bf81789"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_12 False\n",
      "max_pooling2d_12 False\n",
      "conv2d_13 False\n",
      "max_pooling2d_13 False\n",
      "flatten_5 False\n",
      "dense_10 True\n",
      "dense_11 True\n"
     ]
    }
   ],
   "source": [
    "for layer in classifier1.layers:\n",
    "    print(layer.name,layer.trainable)\n",
    "# False indicating that these layers can't be trainable (i.e the weights are freezed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305acd2d",
   "metadata": {
    "id": "305acd2d",
    "outputId": "534ecbf1-0972-4ff1-cd3c-a326f235f8d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 26, 26, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 11, 11, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                51264     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 61,514\n",
      "Trainable params: 56,714\n",
      "Non-trainable params: 4,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier1.summary()\n",
    "# Non trainable parameters = convolution layer parameters = 160 +4640 =4800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd52fef",
   "metadata": {
    "id": "9dd52fef",
    "outputId": "6c00c55d-91d9-4abc-9d2a-64ca24657c28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/10\n",
      "42000/42000 [==============================] - 20s 483us/sample - loss: 0.8078 - accuracy: 0.7591 - val_loss: 0.5641 - val_accuracy: 0.8397\n",
      "Epoch 2/10\n",
      "42000/42000 [==============================] - 20s 474us/sample - loss: 0.4907 - accuracy: 0.8580 - val_loss: 0.4747 - val_accuracy: 0.8641\n",
      "Epoch 3/10\n",
      "42000/42000 [==============================] - 20s 466us/sample - loss: 0.4122 - accuracy: 0.8814 - val_loss: 0.4318 - val_accuracy: 0.8763\n",
      "Epoch 4/10\n",
      "42000/42000 [==============================] - 18s 417us/sample - loss: 0.3624 - accuracy: 0.8931 - val_loss: 0.4060 - val_accuracy: 0.8837\n",
      "Epoch 5/10\n",
      "42000/42000 [==============================] - 17s 410us/sample - loss: 0.3251 - accuracy: 0.9035 - val_loss: 0.3939 - val_accuracy: 0.8880\n",
      "Epoch 6/10\n",
      "42000/42000 [==============================] - 17s 416us/sample - loss: 0.2943 - accuracy: 0.9127 - val_loss: 0.3880 - val_accuracy: 0.8917\n",
      "Epoch 7/10\n",
      "42000/42000 [==============================] - 17s 412us/sample - loss: 0.2692 - accuracy: 0.9196 - val_loss: 0.3872 - val_accuracy: 0.8885\n",
      "Epoch 8/10\n",
      "42000/42000 [==============================] - 19s 442us/sample - loss: 0.2461 - accuracy: 0.9266 - val_loss: 0.3798 - val_accuracy: 0.8962\n",
      "Epoch 9/10\n",
      "42000/42000 [==============================] - 18s 428us/sample - loss: 0.2273 - accuracy: 0.9322 - val_loss: 0.3801 - val_accuracy: 0.8960\n",
      "Epoch 10/10\n",
      "42000/42000 [==============================] - 18s 417us/sample - loss: 0.2113 - accuracy: 0.9369 - val_loss: 0.3811 - val_accuracy: 0.8948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x233354dbba8>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model. In this step only the weights of dense layers will be updated\n",
    "# convolution layer weights remain same(same as MNIST model weights)\n",
    "classifier1.fit(xtrain1,ytrain,batch_size=32,epochs=10,validation_data=(xtest1,ytest))\n",
    "# Model performance improved drastically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33be2c88",
   "metadata": {
    "id": "33be2c88",
    "outputId": "9aea24d7-a6c1-4d1f-bf1c-3e9e2ce9d0aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_12 False\n",
      "max_pooling2d_12 False\n",
      "conv2d_13 False\n",
      "max_pooling2d_13 False\n",
      "flatten_5 False\n",
      "dense_10 True\n",
      "dense_11 True\n",
      "After unfreezing last convolution layer\n",
      "conv2d_12 False\n",
      "max_pooling2d_12 False\n",
      "conv2d_13 True\n",
      "max_pooling2d_13 True\n",
      "flatten_5 True\n",
      "dense_10 True\n",
      "dense_11 True\n"
     ]
    }
   ],
   "source": [
    "# The approach is along with tuning dense layer weights, we can also retrain few last convolution layers\n",
    "#This is know as Transfer Learning through Fine Tuning\n",
    "for layer in classifier1.layers:\n",
    "    print(layer.name,layer.trainable)\n",
    "classifier1.layers[2].trainable=True\n",
    "classifier1.layers[3].trainable=True\n",
    "classifier1.layers[4].trainable=True\n",
    "print(\"After unfreezing last convolution layer\")\n",
    "for layer in classifier1.layers:\n",
    "    print(layer.name,layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9dd04d",
   "metadata": {
    "id": "1d9dd04d",
    "outputId": "a83f004a-3ffb-4182-9c43-d752720ca43e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/10\n",
      "42000/42000 [==============================] - 20s 474us/sample - loss: 0.1931 - accuracy: 0.9424 - val_loss: 0.4056 - val_accuracy: 0.8972\n",
      "Epoch 2/10\n",
      "42000/42000 [==============================] - 21s 505us/sample - loss: 0.1798 - accuracy: 0.9463 - val_loss: 0.3997 - val_accuracy: 0.8971\n",
      "Epoch 3/10\n",
      "42000/42000 [==============================] - 20s 487us/sample - loss: 0.1643 - accuracy: 0.9510 - val_loss: 0.4052 - val_accuracy: 0.8979\n",
      "Epoch 4/10\n",
      "42000/42000 [==============================] - 17s 416us/sample - loss: 0.1530 - accuracy: 0.9539 - val_loss: 0.4206 - val_accuracy: 0.8964\n",
      "Epoch 5/10\n",
      "42000/42000 [==============================] - 17s 410us/sample - loss: 0.1402 - accuracy: 0.9576 - val_loss: 0.4326 - val_accuracy: 0.8972\n",
      "Epoch 6/10\n",
      "42000/42000 [==============================] - 19s 448us/sample - loss: 0.1297 - accuracy: 0.9614 - val_loss: 0.4577 - val_accuracy: 0.8959\n",
      "Epoch 7/10\n",
      "42000/42000 [==============================] - 17s 416us/sample - loss: 0.1233 - accuracy: 0.9635 - val_loss: 0.4678 - val_accuracy: 0.8927\n",
      "Epoch 8/10\n",
      "42000/42000 [==============================] - 18s 437us/sample - loss: 0.1130 - accuracy: 0.9657 - val_loss: 0.4988 - val_accuracy: 0.8933\n",
      "Epoch 9/10\n",
      "42000/42000 [==============================] - 17s 410us/sample - loss: 0.1029 - accuracy: 0.9696 - val_loss: 0.4995 - val_accuracy: 0.8928\n",
      "Epoch 10/10\n",
      "42000/42000 [==============================] - 17s 410us/sample - loss: 0.0964 - accuracy: 0.9704 - val_loss: 0.5429 - val_accuracy: 0.8906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x23336854e10>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model. In this step along with all the dense layers, last convolution layer \n",
    "                                                           #weights also will be updated\n",
    "classifier1.fit(xtrain1,ytrain,batch_size=32,epochs=10,validation_data=(xtest1,ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d70faf",
   "metadata": {
    "id": "d3d70faf"
   },
   "outputs": [],
   "source": [
    "# Reduce overfitting - Include dropout layer\n",
    "classifier1=Sequential()\n",
    "\n",
    "classifier1.add(Conv2D(16,(3,3),input_shape=(28,28,1),activation ='relu'))\n",
    "classifier1.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "classifier1.add(Conv2D(32,(3,3),activation ='relu'))\n",
    "classifier1.add(MaxPooling2D(pool_size=(2,2)))\n",
    "classifier1.add(Dropout(0.2))\n",
    "\n",
    "classifier1.add(Flatten())\n",
    "\n",
    "classifier1.add(Dense(units=64,activation='relu'))\n",
    "classifier1.add(Dropout(0.2))\n",
    "classifier1.add(Dense(units=10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e779bd5e",
   "metadata": {
    "id": "e779bd5e",
    "outputId": "a9ce3220-bfdb-4692-c568-7eab27f9ab9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_8 True\n",
      "max_pooling2d_8 True\n",
      "conv2d_9 True\n",
      "max_pooling2d_9 True\n",
      "dropout_2 True\n",
      "flatten_3 True\n",
      "dense_6 True\n",
      "dropout_3 True\n",
      "dense_7 True\n"
     ]
    }
   ],
   "source": [
    "for layer in classifier1.layers:\n",
    "    print(layer.name,layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77d59d5",
   "metadata": {
    "id": "a77d59d5"
   },
   "outputs": [],
   "source": [
    "classifier1.load_weights('my_digit_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7bfc0a",
   "metadata": {
    "id": "1f7bfc0a"
   },
   "outputs": [],
   "source": [
    "for layer in classifier1.layers:\n",
    "    if ('dense' not in layer.name):\n",
    "        layer.trainable=False\n",
    "    if('dense' in layer.name):\n",
    "        layer.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6fefe8",
   "metadata": {
    "id": "eb6fefe8"
   },
   "outputs": [],
   "source": [
    "classifier1.layers[2].trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210725e0",
   "metadata": {
    "id": "210725e0"
   },
   "outputs": [],
   "source": [
    "classifier1.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5250810d",
   "metadata": {
    "id": "5250810d",
    "outputId": "a7345375-5fad-47a0-c92b-8c4979a2533a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "42000/42000 [==============================] - 15s 357us/sample - loss: 0.3726 - accuracy: 0.8846 - val_loss: 0.3525 - val_accuracy: 0.8967\n",
      "Epoch 2/5\n",
      "42000/42000 [==============================] - 15s 359us/sample - loss: 0.3566 - accuracy: 0.8882 - val_loss: 0.3420 - val_accuracy: 0.9003\n",
      "Epoch 3/5\n",
      "42000/42000 [==============================] - 15s 361us/sample - loss: 0.3501 - accuracy: 0.8887 - val_loss: 0.3416 - val_accuracy: 0.9018\n",
      "Epoch 4/5\n",
      "42000/42000 [==============================] - 15s 360us/sample - loss: 0.3441 - accuracy: 0.8914 - val_loss: 0.3500 - val_accuracy: 0.8983\n",
      "Epoch 5/5\n",
      "42000/42000 [==============================] - 13s 309us/sample - loss: 0.3333 - accuracy: 0.8946 - val_loss: 0.3400 - val_accuracy: 0.8995\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2333682a080>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier1.fit(xtrain1,ytrain,batch_size=32,epochs=5,validation_data=(xtest1,ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5b04ce",
   "metadata": {
    "id": "7a5b04ce"
   },
   "outputs": [],
   "source": [
    "#including dropout layer reducing the overfitting "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Faculty_Notebook_Session6_Transfer_Learning_workout1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
